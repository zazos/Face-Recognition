{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives Calculation 1.1\n",
    "\n",
    "The function $f : \\mathbb{R}^d \\to \\mathbb{R}$ is defined as:\n",
    "\n",
    "$$\n",
    "f(x) = (x-a)^T A(x-a), \\quad where \\: A \\in \\mathbb{R}^{d \\times d} \\: symmetric (A^T = A)\n",
    "$$\n",
    "\n",
    "We can rewrite the function using $x_i$ and $a_i$ to denote the components of $x$ and $a$ respecively, as well as $A_{ij}$ to be the elements of matrix $A$:\n",
    "$$\n",
    "f(x) = (x_i - a_i)^T A_{ij}(x_j - a_j)\n",
    "$$\n",
    "\n",
    "$f(x)$ with respect to each $x_k$ is: \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left[ \\sum_{i=1}^d \\sum_{j=1}^d (x_i - a_i)A_{ij}(x_j - a_j) \\right]\n",
    "$$\n",
    "\n",
    "1. When $ i=k $ kai $ j \\neq k $ : $ \\frac{\\partial}{\\partial x_k} ( x_i - a_i ) = 1 $ and $ (x_j - a_j) $ constant.\n",
    "So using the product rule gives us: \n",
    "$$\n",
    "\\frac{\\partial (f(x)g(x))}{\\partial x} = f'(x)g(x) + f(x)g'(x) = (x_i - a_i)'A_{ij}(x_j - a_j) + (x_i - a_i)A_{ij}(x_j - a_j)' = A_{ij}(x_j - a_j)\n",
    "$$\n",
    "\n",
    "2. When $ i \\neq k $ and $ j=k $ : The procedure is the same as above, resulting to: $ A_{ij}(x_i - a_i) $\n",
    "\n",
    "Due to symmetricality, $ A_{ij} = A_{ji} $, for every $ (i, j) $ with $ i \\neq j $, we have the combined: \n",
    "$$ \n",
    "A_{ik}(x_i - a_i) + A_{ki}(x_k - a_k) = 2A_{ik}(x_i - a_i) \n",
    "$$\n",
    "\n",
    "Additionaly, when $i=j=k$ the product simplifies to $ (x_k - a_k)A_{kk}(x_k - a_k) $, which can be written as $ A_{kk}(x_k - a_k)^2 $\n",
    "\n",
    "Finally, we get \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_k} = 2 \\sum_{i=1}^d A_{ik}(x_i - a_i)\n",
    "$$\n",
    "\n",
    "Therefore for every k:\n",
    "$$\n",
    "\\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\: ... \\:, \\frac{\\partial f}{\\partial x_k} \\right] = 2A(x-a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives Calculation 1.2\n",
    "\n",
    "In order to find the matrix $ X $ which minimizes the Frobenius norm of $ A $ and $ XB $, we must take the derivative of the function with respect to $ X $, set it to zero and solve for $ X $.\n",
    "\n",
    "To begin with, the Frobenius norm: \n",
    "$$\n",
    "f(x) = \\|A - XB\\|_F^2\n",
    "$$\n",
    "\n",
    "By expanding this norm, we get:\n",
    "$$\n",
    "f(x) = trace((A-XB)^T(A-XB))\n",
    "$$\n",
    "\n",
    "The trace is the sum of the elements on the main diagonal, and for a square $ n x n $ matrix $ M $, it's defined as: \n",
    "$$ trace(M) = \\sum_{i=1}^n m_{ii} $$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "trace((A-XB)_T(A-XB)) = trace(A^A - A^TXB - (XB)^TA + (XB)^TXB)\n",
    "$$\n",
    "Due to trace's linearity and cyclic permutations of matrix products ( $ trace(AB) = trace(BA) $ ), we get that $ trace(A^TXB) = trace(XBA^T) $. \n",
    "So\n",
    "$$\n",
    "trace((A-XB)^T(A-XB)) = trace(A^TA - 2A^TXB + (XB)^TXB)\n",
    "$$\n",
    "\n",
    "Now we take the derivative of $ f(X) $ with respect to $ X $:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial X} = \\frac{\\partial}{\\partial X} \\left [trace(A^TA) - 2trace(A^TXB) + trace((XB)^TXB)) \\right]\n",
    "$$\n",
    "\n",
    "- $ trace(A^TA) = 0 $ due to $ A $ being a constant matrix\n",
    "- $ trace(A^TXB) = B^TA $, as we simply take the constant matrices transposed and multiplied in the opposite order\n",
    "- $ trace((XB)^TXB) = 2XB^TB $, as we can rewrite the trace as $ trace(X^TXBB^T) $. It's known that the derivative of $ trace(X^XC) $ with respect to $ X $, where $ C $ is a constant matrix, is $ 2XC $, when C is symmetric (and $ B^TB $ is symmetric )\n",
    "\n",
    "After combining the aforementioned derivatives and setting the gradient to zero in order to find the minimum of $ f(X) $ we get:\n",
    "$$\n",
    "-2B^TA + 2XB^TB = 0 <==> B^TXB = B^TA <==> X = \\frac{B^TA}{B^TB} <==> X = (B^TB)^{-1}B^TA\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Calculation for $f_1$ with $L_1$\n",
    "\n",
    "The function $ f_1(x; w) $ (Logistic regression model) is defined as:\n",
    "\n",
    "$$\n",
    "f_1(x; w) = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}\n",
    "$$\n",
    "\n",
    "Binary cross-entropy loss, $L_1$:\n",
    "\n",
    "$$\n",
    "L_1(w) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log f_1(x_i; w) + (1 - y_i) \\log (1 - f_1(x_i; w)) \\right]\n",
    "$$\n",
    "\n",
    "#### Derivative of the Sigmoid Function\n",
    "(https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e)\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "#### Computation of $L_1$ gradient with respect to each weight $w_j$\n",
    "\n",
    "The partial derivatives of the logarithmic components of $L_1$ are as follows:\n",
    "$$\n",
    "\\frac{\\partial \\log(f_1(x; w))}{\\partial w_j} = \\frac{1}{f_1(x; w)} \\frac{\\partial f_1(x; w)}{\\partial w_j}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\log(1 - f_1(x; w))}{\\partial w_j} = \\frac{-1}{1 - f_1(x; w)} \\frac{\\partial f_1(x; w)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "\n",
    "Considering $f_1(x; w) = \\sigma(w^T x)$, the derivative with respect to $w_j$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma(w^T x)}{\\partial w_j} = \\sigma(w^T x)(1 - \\sigma(w^T x)) x_j = f_1(x; w)(1 - f_1(x; w)) x_j\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, the gradient of the loss function with respect to $w_j$ becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_1}{\\partial w_j} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\frac{1}{f_1(x_i; w)} - (1 - y_i) \\frac{1}{1 - f_1(x_i; w)} \\right] f_1(x_i; w)(1 - f_1(x_i; w)) x_{ji}\n",
    "$$\n",
    "\n",
    "### Simplify each term\n",
    "\n",
    "- **Term $y_i$:**\n",
    "  $$\n",
    "  y_i \\frac{1}{f_1(x_i; w)} \\cdot f_1(x_i; w)(1 - f_1(x_i; w))\n",
    "  $$\n",
    "  simplifies to:\n",
    "  $$\n",
    "  y_i (1 - f_1(x_i; w))\n",
    "  $$\n",
    "\n",
    "- **Term $1-y_i$:**\n",
    "  $$\n",
    "  -(1 - y_i) \\frac{1}{1 - f_1(x_i; w)} \\times f_1(x_i; w)(1 - f_1(x_i; w))\n",
    "  $$\n",
    "  simplifies to:\n",
    "  $$\n",
    "  -(1 - y_i) f_1(x_i; w)\n",
    "  $$\n",
    "\n",
    "### Combining and multiplying with the sigmoid\n",
    "\n",
    "$$\n",
    "  \\left[y_i (1 - f_1(x_i; w)) -(1 - y_i) f_1(x_i; w) \\right]x_{ji}\n",
    "$$\n",
    "which leaves us with\n",
    "$$\n",
    "y_i (1 - f_1(x_i; w)) - (1 - y_i) f_1(x_i; w) = y_i - y_i f_1(x_i; w) - f_1(x_i; w) + y_i f_1(x_i; w)\n",
    "$$\n",
    "and finally \n",
    "$$\n",
    "y_i - f_1(x_i; w)\n",
    "$$\n",
    "\n",
    "### Finally, the gradient expression\n",
    "$$\n",
    "\\frac{\\partial L_1}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n (f_1(x_i; w) - y_i) x_{ji}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def linear(x, w):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def f1(x, w):\n",
    "    return sigmoid(np.dot(x, w))\n",
    "\n",
    "def L1(w, X, y):\n",
    "    predictions = f1(X, w)\n",
    "    return -np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions)) / len(X)\n",
    "\n",
    "def grad_L1(w, X, y):\n",
    "    predictions = f1(X, w)\n",
    "    errors = predictions - y\n",
    "    return np.dot(X.T, errors) / len(X)\n",
    "\n",
    "def gradient_descent_f1(X, y, w_init, learning_rate, num_iterations, scheduler=False):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        if scheduler:\n",
    "            if i < 100:\n",
    "                learning_rate = 1e-3\n",
    "            elif i < 140:\n",
    "                learning_rate = 5e-4\n",
    "            else:\n",
    "                learning_rate = 1e-4\n",
    "        learning_rate -= learning_rate *  1e-6\n",
    "                \n",
    "        gradients = grad_L1(w, X, y)\n",
    "        w -= learning_rate * gradients\n",
    "        loss = L1(w, X, y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "            \n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Calculation for $f_2$ with $L_2$\n",
    "\n",
    "The function $ f_2(x; w) $ is defined as:\n",
    "$$\n",
    "f_2(x; w) = w_0 + w_1 \\sigma(w_2 + w_3 x)\n",
    "$$\n",
    "where $ \\sigma $ represents the ReLU function, defined as $ \\sigma(z) = \\max(0, z) $.\n",
    "\n",
    "The mean squared error loss function (MSE) $ L_2(w) $ is given by:\n",
    "$$ \n",
    "L_2(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_2(x_i; w))^2\n",
    "$$\n",
    "\n",
    "### Gradient Calculations\n",
    "\n",
    "#### **1. Derivative with respect to $ w_0 $**\n",
    "$$\n",
    "\\frac{\\partial L_2}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^n (f_2(x_i; w) - y_i) \n",
    "$$\n",
    "\n",
    "#### **2. Derivative with respect to $ w_1 $**\n",
    "The derivative of $ f_2(x; w) $ with respect to $ w_1 $ is:\n",
    "$$\n",
    "\\frac{\\partial f_2}{\\partial w_1} = \\sigma(w_2 + w_3 x)\n",
    "$$\n",
    "And then apply $ f_2 $ derivative to the derivative of $ L_2 $\n",
    "$$\n",
    "\\frac{\\partial L_2}{\\partial w_1} = \\frac{2}{n} \\sum_{i=1}^n (f_2(x_i; w) - y_i) \\sigma(w_2 + w_3 x_{i})\n",
    "$$\n",
    "\n",
    "#### **3. Derivative with respect to $ w_2 $ and $ w_3 $**\n",
    "Since $ \\sigma'(z) = 1 $ if z > 0 and 0 otherwise:\n",
    "$$\n",
    "\\frac{\\partial f_2}{\\partial w_2} = \\frac{\\partial f_2}{\\partial w_3} = \\left[ w_2 + w_3 x_{i} > 0 \\right]\n",
    "$$\n",
    "And then inserting these derivatives to $ L_2 $ derivative:\n",
    "$$\n",
    "\\frac{\\partial L_2}{\\partial w_2} = \\frac{2}{n} \\sum_{i=1}^n (f_2(x_i; w) - y_i)w_1 \\left[ w_2 + w_3 x_{i} > 0 \\right]\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L_2}{\\partial w_3} = \\frac{2}{n} \\sum_{i=1}^n (f_2(x_i; w) - y_i)w_1 x_{i} \\left[ w_2 + w_3 x_{i} > 0 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(s):\n",
    "    return np.maximum(0, s)\n",
    "\n",
    "def relu_prime(s):\n",
    "    return np.where(s > 0, 1, 0)\n",
    "\n",
    "def f2(X, w):\n",
    "    return w[0] + w[1] * relu(w[2] + w[3] * X)\n",
    "\n",
    "def L2(w, X, y):\n",
    "    predictions = f2(X, w)\n",
    "    return np.sum((y - predictions) ** 2) / len(X)\n",
    "\n",
    "def grad_L2(w, X, y):\n",
    "    predictions = f2(X, w)\n",
    "    error = predictions - y\n",
    "    relu_input = w[2] + w[3] * X\n",
    "    relu_derivative = relu_prime(relu_input)\n",
    "\n",
    "    grad_w0 = np.sum(2 * error) / len(X)\n",
    "\n",
    "    grad_w1 = np.sum(2 * error * relu(relu_input)) / len(X)\n",
    "    grad_w2 = np.sum(2 * error * w[1] * relu_derivative) / len(X)\n",
    "    grad_w3 = np.sum(2 * error * w[1] * X * relu_derivative) / len(X)\n",
    "\n",
    "    return np.array([grad_w0, grad_w1, grad_w2, grad_w3])\n",
    "\n",
    "\n",
    "def gradient_descent_f2(X, y, w_init, learning_rate, num_iterations, scheduler=False):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        if scheduler:\n",
    "            if i < 100:\n",
    "                learning_rate = 1e-3\n",
    "            elif i < 140:\n",
    "                learning_rate = 5e-4\n",
    "            else:\n",
    "                learning_rate = 1e-4\n",
    "            learning_rate -= learning_rate *  1e-6\n",
    "            \n",
    "        gradients = grad_L2(w, X, y)\n",
    "        w -= learning_rate * gradients\n",
    "        loss = L2(w, X, y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "            \n",
    "    return w, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.round(losses, 4), label='Loss value')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_f1(X, y, w):\n",
    "    plt.scatter(X[:, 1], y, c=y, cmap='viridis', edgecolor='k', label='Training data')\n",
    "    x_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 300)\n",
    "    x_range_with_bias = np.hstack((np.ones((x_range.shape[0], 1)), x_range.reshape(-1, 1)))  # Add bias term\n",
    "    predictions = f1(x_range_with_bias, w)\n",
    "\n",
    "    plt.plot(np.round(x_range, 4), np.round(predictions, 4), label='Model predictions', color='red')\n",
    "    plt.xlabel('Input Values')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title('Training Data and Model Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_f2(X, y, w):\n",
    "    plt.scatter(X, y, c=y, cmap='viridis', edgecolor='k', label='Training data')\n",
    "    x_range = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
    "    predictions = f2(x_range, w)\n",
    "\n",
    "    plt.plot(np.round(x_range, 4), np.round(predictions, 4), label='Model predictions', color='red')\n",
    "    plt.xlabel('Input Values')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title('Training Data and Model Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_f1():\n",
    "    return np.array([0., 1.])\n",
    "\n",
    "def initialize_weights_f2():\n",
    "    return np.array([0.5, 1., 2.2, 3.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.array([[ 10.63415642, -10.31804897,  -3.91323692,  13.47455445,\n",
    "         12.00245098,   7.01459213, -11.5083141 ,   7.83875   ,\n",
    "        -11.21021617,   6.95282804,   7.79101979,  13.94835343,\n",
    "          3.01243432,  11.67679791,  13.41197807,  14.61995534,\n",
    "         -8.70743264, -14.10312612, -11.68244994,  14.75202699,\n",
    "          9.25316629,  -4.82661632,  -6.99915042,   6.49562369,\n",
    "        -11.46126563,  -9.38897405,  -3.68953244,  -5.95603391,\n",
    "        -10.80285714, -11.78004227],\n",
    "       [  1.        ,   0.        ,   0.        ,   1.        ,\n",
    "          1.        ,   1.        ,   0.        ,   1.        ,\n",
    "          0.        ,   1.        ,   1.        ,   1.        ,\n",
    "          1.        ,   1.        ,   1.        ,   1.        ,\n",
    "          0.        ,   0.        ,   0.        ,   1.        ,\n",
    "          1.        ,   0.        ,   0.        ,   1.        ,\n",
    "          0.        ,   0.        ,   0.        ,   0.        ,\n",
    "          0.        ,   0.        ]])\n",
    "\n",
    "data1_input = data1[0].reshape(-1, 1) # convert to 2D array\n",
    "data1_output = data1[1]\n",
    "\n",
    "plt.scatter(data1_input, data1_output)\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel\n",
    "(\"output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.array([[-6.63823962e-01,  1.76259595e+00, -3.99908500e+00,\n",
    "        -1.58133942e+00, -2.82595287e+00, -3.26129124e+00,\n",
    "        -2.50991831e+00, -1.23551418e+00, -8.25860206e-01,\n",
    "         3.10533872e-01, -6.46443885e-01,  1.48175600e+00,\n",
    "        -2.36438200e+00,  3.02493949e+00, -3.78089925e+00,\n",
    "         1.36374008e+00, -6.61561581e-01,  4.69518628e-01,\n",
    "        -2.87690449e+00, -2.41518809e+00,  2.40595655e+00,\n",
    "         3.74609261e+00, -1.49260657e+00,  1.53858093e+00,\n",
    "         3.01111322e+00,  3.15685331e+00, -3.31964631e+00,\n",
    "        -3.68756173e+00, -2.64135664e+00,  3.02514003e+00],\n",
    "       [-7.76859909e-02,  6.84885863e+00,  2.11068734e-02,\n",
    "         2.91407607e-01, -5.50309589e-01,  5.72361855e-01,\n",
    "         4.50795360e-01,  2.51247169e-01,  4.50427975e-01,\n",
    "         2.58973769e+00, -7.76767080e-04,  5.97738329e+00,\n",
    "        -1.33944040e-01,  1.13399962e+01, -3.45830376e-01,\n",
    "         5.89284348e+00, -3.28271093e-01,  2.98595306e+00,\n",
    "        -3.35623065e-01, -6.33229946e-03,  8.65921447e+00,\n",
    "         1.33554857e+01,  8.29901089e-01,  6.98676486e+00,\n",
    "         1.09374219e+01,  1.10267454e+01, -3.73579147e-01,\n",
    "         8.46227301e-01,  2.54038774e-02,  1.07569223e+01]])\n",
    "data2_input = data2[0]\n",
    "data2_output = data2[1]\n",
    "\n",
    "plt.scatter(data2_input, data2_output)\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "NUM_ITER = 150\n",
    "\n",
    "w_init_f1 = initialize_weights_f1()\n",
    "# adding bias to input data for dot product to be possible in f1\n",
    "data1_input = np.hstack((np.ones((data1_input.shape[0], 1)), data1_input))\n",
    "final_weights_f1, losses_f1 = gradient_descent_f1(data1_input, data1_output, w_init_f1, LR, NUM_ITER)\n",
    "\n",
    "plot_learning_curve(losses_f1)\n",
    "plot_predictions_f1(data1_input, data1_output, final_weights_f1)\n",
    "\n",
    "w_init_f2 = initialize_weights_f2()\n",
    "final_weights_f2, losses_f2 = gradient_descent_f2(data2_input, data2_output, w_init_f2, LR, NUM_ITER)\n",
    "\n",
    "plot_learning_curve(losses_f2)\n",
    "plot_predictions_f2(data2_input, data2_output, final_weights_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loss value for the Sigmoid model tells us that since its first iteration, it has reached its plateau value, and thus further iterations do not improve the model at all. This is probably due to the model's finding the best fit given the current data.\n",
    "\n",
    "#### On the other hand, the loss value of the Relu model starts off with a high loss value and rapidly decreasing, showing effective learning cabalities in early iterations. But at approximetaly 20-40 iterations, the model minimized the loss value as best as it could and plateaued.\n",
    "\n",
    "#### The Sigmoid's predictions show  the model's capabilities of fitting a sigmoid function effectively across the data points, being able to distinguish the two groups.\n",
    "\n",
    "#### Relu's prediction are reasonably good as well, where a few outlies that weren't predicted perfectly, which shows the potential area for improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "NUM_ITER = 150\n",
    "\n",
    "final_weights_f1, losses_f1 = gradient_descent_f1(data1_input, data1_output, w_init_f1, LR, NUM_ITER)\n",
    "\n",
    "plot_learning_curve(losses_f1)\n",
    "plot_predictions_f1(data1_input, data1_output, final_weights_f1)\n",
    "\n",
    "final_weights_f2, losses_f2 = gradient_descent_f2(data2_input, data2_output, w_init_f2, LR, NUM_ITER)\n",
    "\n",
    "plot_learning_curve(losses_f2)\n",
    "plot_predictions_f2(data2_input, data2_output, final_weights_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "NUM_ITER = 150\n",
    "\n",
    "final_weights_f1, losses_f1 = gradient_descent_f1(data1_input, data1_output, w_init_f1, LR, NUM_ITER, scheduler=True)\n",
    "\n",
    "plot_learning_curve(losses_f1)\n",
    "plot_predictions_f1(data1_input, data1_output, final_weights_f1)\n",
    "\n",
    "final_weights_f2, losses_f2 = gradient_descent_f2(data2_input, data2_output, w_init_f2, LR, NUM_ITER, scheduler=True)\n",
    "\n",
    "plot_learning_curve(losses_f2)\n",
    "plot_predictions_f2(data2_input, data2_output, final_weights_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_f1(X, y, w_init, learning_rate, num_iterations, batch_size):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch = np.random.choice(len(X), batch_size, replace=False)\n",
    "        X_batch = X[batch]\n",
    "        y_batch = y[batch]\n",
    "        \n",
    "        gradients = grad_L1(w, X_batch, y_batch)\n",
    "        w -= learning_rate * gradients\n",
    "        \n",
    "        loss = L1(w, X, y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "    \n",
    "    return w, losses\n",
    "        \n",
    "def stochastic_gradient_descent_f2(X, y, w_init, learning_rate, num_iterations, batch_size):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch = np.random.choice(len(X), batch_size, replace=False)\n",
    "        X_batch = X[batch]\n",
    "        y_batch = y[batch]\n",
    "        \n",
    "        gradients = grad_L2(w, X_batch, y_batch)\n",
    "        w -= learning_rate * gradients\n",
    "        \n",
    "        loss = L2(w, X, y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "    \n",
    "    return w, losses\n",
    "\n",
    "def plot_learning_curve_stochastic(losses, batch_size):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.round(losses, 4), label='Loss value rounded (3 decimals)')\n",
    "    plt.title(f'Learning Curve for batch {batch_size}')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "BATCH_SIZES = [3, 10, 15]\n",
    "LR = 1e-3\n",
    "\n",
    "for batch_size in BATCH_SIZES:\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    final_weights_f1, losses_f1 = stochastic_gradient_descent_f1(data1_input, data1_output, w_init_f1, LR, NUM_ITER, batch_size)\n",
    "    plot_learning_curve_stochastic(losses_f1, batch_size)\n",
    "    plot_predictions_f1(data1_input, data1_output, final_weights_f1)\n",
    "\n",
    "    final_weights_f2, losses_f2 = stochastic_gradient_descent_f2(data2_input, data2_output, w_init_f2, LR, NUM_ITER, batch_size)\n",
    "    plot_learning_curve_stochastic(losses_f2, batch_size)\n",
    "    plot_predictions_f2(data2_input, data2_output, final_weights_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Sigmoid model it seems that for batch 3, the sharp decrease in loss could probably be due to encountering a significant feature in the data. But for batches 10 and 15, the flat loss signifies that either that's the best the model can do given the data or that the learning rate might have been too low to make significant updates.\n",
    "#### For its predictions, the model effectively predicts the actual data points.\n",
    "\n",
    "#### For the Relu model, the leraning curves improve as we iterate, indicating that learning rate and the batch selection are effective. Moreover we can also observe that the fluctuations in the loss, from batch 3 to batch 15, begin more chaotic and end at a relative stale curve (this occurs due to the fact that SGD randomly selects batch of data and introduces this kind of noise). \n",
    "#### The variations in the learning curves suggest that batch composition can impact the smoothness and the stability of the learning process, which highlights the importance of LR and batch_size\n",
    "#### As for the predictions, the models effectively predicts, as the Sigmoid model, the actual data points, not much different that the regular gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path, set_number):\n",
    "    set_ranges = {\n",
    "        'Set_1': range(1, 8),\n",
    "        'Set_2': range(8, 20),\n",
    "        'Set_3': range(20, 32),\n",
    "        'Set_4': range(32, 46),\n",
    "        'Set_5': range(46, 65)\n",
    "    }\n",
    "    \n",
    "    image_matrix = []\n",
    "    labels = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        person_image = filename.split('_')\n",
    "        person_id = int(person_image[0].replace('person', ''))\n",
    "        image_number = int(person_image[1].split('.')[0])\n",
    "         \n",
    "        if image_number in set_ranges[set_number]:\n",
    "            # https://www.geeksforgeeks.org/python-pil-imageops-greyscale-method/\n",
    "            img = ImageOps.grayscale(Image.open(os.path.join(path, filename)))\n",
    "            img_data = np.array(img).flatten() # 1D array\n",
    "            image_matrix.append(img_data)\n",
    "            labels.append(person_id)\n",
    "    \n",
    "    image_matrix = np.array(image_matrix).T # store images as columns\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return image_matrix, labels\n",
    "    \n",
    "!unzip -o faces.zip    \n",
    "image_matrix, labels = loadImages('./faces', 'Set_2')\n",
    "print(\"Image matrix shape:\", image_matrix.shape)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usage of StandardScaler to subtract the mean (mean centering) and divide by the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eigenfaces(image_matrix, n_components=30):\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "    scaler = StandardScaler()\n",
    "    standarized_image_matrix = scaler.fit_transform(image_matrix.T) # expects samples as rows, features as columns, so we transpose the image matrix\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(standarized_image_matrix) # fit on images as rows\n",
    "    return pca, scaler\n",
    "\n",
    "def reconstruct_image(pca, scaler, input_image):\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform\n",
    "    standarized_image = scaler.transform(input_image.reshape(1, -1)) # flat array to 2D array\n",
    "    transformed_image = pca.transform(standarized_image)\n",
    "    reconstructed_image = pca.inverse_transform(transformed_image)\n",
    "    unstandardized_reconstructed_image = scaler.inverse_transform(reconstructed_image)\n",
    "    \n",
    "    # Show original and reconstructed images\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(input_image.reshape(50, 50), cmap='gray')\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[1].imshow(reconstructed_image.reshape(50, 50), cmap='gray')\n",
    "    axes[1].set_title('Reconstructed Image')\n",
    "    plt.show()\n",
    "    \n",
    "    return unstandardized_reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "image_matrix, labels = loadImages('./faces', 'Set_1')\n",
    "pca, scaler = train_eigenfaces(image_matrix, n_components=30)\n",
    "random_number = random.randint(0, image_matrix.shape[1] - 1)\n",
    "print(f\"Reconstructing image number {random_number}\")\n",
    "reconstructed_image = reconstruct_image(pca, scaler, image_matrix[:, random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reconstructed image lacks of finer details. It seems to capture the most significant variations across the set of images that the eigenfaces were trained on. And despite the detail loss, the major features of the face, such as the position of eyes, nose, lips and eyebrows are still recognizable and well centered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_pca(image_matrix, n_components):\n",
    "    pca, scaler = train_eigenfaces(image_matrix, n_components)\n",
    "    \n",
    "    standardized_data = scaler.transform(image_matrix.T)\n",
    "    transformed_image = pca.transform(standardized_data)\n",
    "    reconstructed_image = pca.inverse_transform(transformed_image)\n",
    "    unstandardized_data = scaler.inverse_transform(reconstructed_image)\n",
    "\n",
    "    mse = np.mean((image_matrix.T - unstandardized_data) ** 2)\n",
    "    \n",
    "    return pca, scaler, mse\n",
    "\n",
    "errors = []\n",
    "d_values = sample(range(2, image_matrix.shape[1] - 1), 10)\n",
    "# d values must be between 2 and min(n_samples, n_features) = max images of given Set\n",
    "pca_models = {}\n",
    "\n",
    "for d in d_values:\n",
    "    pca, scaler, mse = train_evaluate_pca(image_matrix, d)\n",
    "    errors.append(mse)\n",
    "    pca_models[d] = (pca, scaler)\n",
    "\n",
    "# Plotting errors using a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(d_values, errors, color='skyblue')\n",
    "plt.xlabel('Number of components (d)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Reconstruction Error for d values of PCA Components')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_eigenfaces(image_matrix, n_components=9):\n",
    "    pca, _ = train_eigenfaces(image_matrix, n_components)\n",
    "    eigenfaces = pca.components_.reshape((n_components, 50, 50)) # reshape to 50x50 images for visualization\n",
    "    _, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(eigenfaces[i], cmap='gray')\n",
    "        ax.set_title(f\"Eigenface {i+1}\")\n",
    "    plt.show()\n",
    "    \n",
    "image_matrix, labels = loadImages('./faces', 'Set_1')\n",
    "top_eigenfaces(image_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each eigenface is actually an eigenvector (\"standarized face ingredients\"), even though it seems as an image. A representation of pixels is an image. Moreover, each eigenface represents a mode of variation among the images of Set1. These modes are the variations in which the images vary the most. \n",
    "### Eigenface1 captures the most general features and the average lightning. Progressively, the vectors capture finer details and changes in lightning from different angles, variations in facial expressions and many other attributes like a mustaches, frowned eyebrows and such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_classification(pca, scaler, X_train, y_train, X_test, y_test):\n",
    "    X_train_standardized = scaler.fit_transform(X_train.T)\n",
    "    X_test_standardized = scaler.transform(X_test.T)\n",
    "    \n",
    "    X_train_pca = pca.transform(X_train_standardized)\n",
    "    X_test_pca = pca.transform(X_test_standardized)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    \n",
    "    predictions = clf.predict(X_test_pca)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    return accuracy    \n",
    "\n",
    "def analyze_pca(pca_models, sets):\n",
    "    train_image_matrix, train_labels = loadImages('./faces', 'Set_1')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    color_map = {d: color for d, color in zip(sorted(pca_models.keys()), colors)} \n",
    "    \n",
    "    for set_name in sets:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        d_values = sorted(pca_models.keys())\n",
    "        accuracies = []\n",
    "        \n",
    "        for d, (pca, scaler) in pca_models.items():\n",
    "            test_image_matrix, test_labels = loadImages('./faces', set_name)\n",
    "            accuracy = pca_classification(pca, scaler, train_image_matrix, train_labels, test_image_matrix, test_labels)\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        plt.bar([str(d) for d in d_values], accuracies, color=[color_map[d] for d in d_values], width=0.35, alpha=0.7, label=f\"Accuracy for {set_name}\")\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            plt.text(i, acc, f\"{acc:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "        plt.xlabel('Number of PCA Components (d)')\n",
    "        plt.ylabel('Classification Accuracy')\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.title(f'PCA Classification Accuracy for {set_name}')\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "sets = ['Set_2', 'Set_3', 'Set_4', 'Set_5']\n",
    "analyze_pca(pca_models, sets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Best performance for Set2 is achieved (generally) across all components (d), mostly the lower values, which suggets that images in Set_2 share significant similarity with the training set, Set_1. However, the characteristics captured by the PCA components from Set_1 do not generalize the model, hence the significantly worse accuracies of Set_4 and Set_5. Set_3 suggests a slight similarity, as accuracy stays close to 50%. These similarities can be interpreted as lightning angles, expressions, facial features etc.\n",
    "\n",
    "#### b)Most consistend number of components (d) seem to be d = 9, although the overall accuracies in Sets 3, 4, and 5 are notably lower.\n",
    "\n",
    "#### c) A good trade off between accuracy and generalizing, as well as model simplicity, based on the observed results from the given sample of components (d) would be d = 9.\n",
    "\n",
    "#### Regarding all sub-queries, though, it's difficult to answer these kind of questions when the sample has a vast range that starts of 2 and has a ceiling of min(n_samples, n_features) = max images of given Set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_matrix, train_labels = loadImages('./faces', 'Set_1')\n",
    "pca, scaler = train_eigenfaces(train_image_matrix, n_components=9)\n",
    "\n",
    "for set in sets:\n",
    "    print(f\"Reconstructing images for {set}\")\n",
    "    set_image_matrix, set_labels = loadImages('./faces', set)\n",
    "    random_number = random.randint(0, image_matrix.shape[1] - 1)\n",
    "    reconstruct_image(pca, scaler, set_image_matrix[:, random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's obvious that, as a way to visualize the lower accuracies shown in the figure previous to this cell, the general shape of the face in consistent across faces, with some features intact as well, but they all lose great details around facial features (eyes, nose, mouth, etc) plus considerable blurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-assignment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_matrix, labels = loadImages('./faces', 'Set_1')\n",
    "scaler = StandardScaler()\n",
    "standarized_matrix = scaler.fit_transform(image_matrix.T)\n",
    "svd = TruncatedSVD(n_components=9)\n",
    "U = svd.fit_transform(standarized_matrix)\n",
    "Vt = svd.components_\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(Vt[i].reshape(50, 50), cmap='gray')\n",
    "    ax.set_title(f'Singular Vector {i+1}')\n",
    "plt.show()\n",
    "top_eigenfaces(image_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both methods essentially perform the same mathematical operations. Their only difference is their starting point, the covariance matrix for PCA and the original matrix for SVD, but they converge when the data matrix has been mean subtracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-assignment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None) # keep all components\n",
    "pca.fit(standarized_matrix)\n",
    "# as explained in https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.explained_variance_ratio_\n",
    "# explained_variance_ratio_ndarray of shape (n_components,): Percentage of variance explained by each of the selected components.\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "for i, variance in enumerate(explained_variance_ratio):\n",
    "    print(f\"Explained Variance for Component {i+1}: {variance:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The eigenface (principal component) values above tells us how much of the total data variance each component captures. The highest the value the more significant it is. I am guessing that gathering enough eigenfaces is very valuable, but when these values start to plateau, it might be better to stop gathering more as this might add noise to the images.\n",
    "#### In our example, as shown in the plot above, the optimal eigenfaces would be around 9-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
